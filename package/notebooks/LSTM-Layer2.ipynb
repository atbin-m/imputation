{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LSTM multiinput-series: https://www.aiproblog.com/index.php/2018/11/13/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "2. LSTM train/test: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "3. Loss plotting: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "4. Overfit/Underfit: https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_stats(ytrue, ypred):\n",
    "    \"\"\"\n",
    "    https://stats.stackexchange.com/questions/142248/difference-between-r-square-and-rmse-in-linear-regression\n",
    "    \n",
    "    https://www.sciencedirect.com/topics/engineering/mean-bias-error\n",
    "    \"\"\"\n",
    "    n = len(ytrue)\n",
    "\n",
    "    # Check that the ytrue and ypred are equal length vector.\n",
    "    assert n == len(ypred)\n",
    "    \n",
    "    # sum squared error\n",
    "    sse = np.sum((ytrue - ypred)**2)\n",
    "    \n",
    "    # root mean square error\n",
    "    rmse = np.sqrt(sse/n)\n",
    "\n",
    "    # total sum of squares\n",
    "    tss = np.sum((ytrue - np.mean(ytrue))**2)\n",
    "    tst = np.sum((ypred - np.mean(ypred))**2)\n",
    "    tstp = tst**0.5\n",
    "    tssp = tss**0.5\n",
    "    \n",
    "    soorat = np.sum((ytrue-np.mean(ytrue))*(ypred-np.mean(ypred)))\n",
    "    \n",
    "    # Rsquare\n",
    "    ##rsqr = 1 - sse/tss\n",
    "    rsqr = (soorat/(tssp*tstp))**2\n",
    "\n",
    "    # Mean biased error\n",
    "    mbe = np.mean(ytrue - ypred)\n",
    "    \n",
    "    # IOAD\n",
    "    num = np.sum((ytrue - ypred)**2)\n",
    "    denom = np.abs(ytrue - ypred) + np.abs(ytrue + ypred)\n",
    "    ioad = 1 - num/np.sum(denom**2)\n",
    "\n",
    "    print(\"RMSE: %1.3f, R^2: %1.3f, MBE: %1.3f, IOAD: %1.3f\"%(rmse, rsqr, mbe, ioad))\n",
    "    \n",
    "    return rmse, rsqr, mbe, ioad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_layer2.csv', \n",
    "                 parse_dates=['DateTime'], index_col='DateTime')\n",
    "train_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "train_df.sort_index(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('test_layer2.csv', \n",
    "                 parse_dates=['DateTime'], index_col='DateTime')\n",
    "test_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Fc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01 01:00:00</th>\n",
       "      <td>0.846773</td>\n",
       "      <td>0.918132</td>\n",
       "      <td>0.875012</td>\n",
       "      <td>0.883520</td>\n",
       "      <td>0.386506</td>\n",
       "      <td>0.730006</td>\n",
       "      <td>1.067198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 01:30:00</th>\n",
       "      <td>0.830835</td>\n",
       "      <td>0.868667</td>\n",
       "      <td>0.902072</td>\n",
       "      <td>0.871775</td>\n",
       "      <td>0.445869</td>\n",
       "      <td>0.735292</td>\n",
       "      <td>1.041434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 07:00:00</th>\n",
       "      <td>0.477854</td>\n",
       "      <td>0.376036</td>\n",
       "      <td>-0.285410</td>\n",
       "      <td>0.523461</td>\n",
       "      <td>0.539703</td>\n",
       "      <td>0.539797</td>\n",
       "      <td>0.391297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 07:30:00</th>\n",
       "      <td>-0.274316</td>\n",
       "      <td>0.163167</td>\n",
       "      <td>-0.431348</td>\n",
       "      <td>0.132327</td>\n",
       "      <td>0.257334</td>\n",
       "      <td>0.278477</td>\n",
       "      <td>1.339480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 08:00:00</th>\n",
       "      <td>-0.233189</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>-0.511897</td>\n",
       "      <td>-0.106105</td>\n",
       "      <td>0.043812</td>\n",
       "      <td>0.067382</td>\n",
       "      <td>1.230345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1         2         3         4  \\\n",
       "DateTime                                                                \n",
       "2013-01-01 01:00:00  0.846773  0.918132  0.875012  0.883520  0.386506   \n",
       "2013-01-01 01:30:00  0.830835  0.868667  0.902072  0.871775  0.445869   \n",
       "2013-01-01 07:00:00  0.477854  0.376036 -0.285410  0.523461  0.539703   \n",
       "2013-01-01 07:30:00 -0.274316  0.163167 -0.431348  0.132327  0.257334   \n",
       "2013-01-01 08:00:00 -0.233189 -0.029411 -0.511897 -0.106105  0.043812   \n",
       "\n",
       "                            5        Fc  \n",
       "DateTime                                 \n",
       "2013-01-01 01:00:00  0.730006  1.067198  \n",
       "2013-01-01 01:30:00  0.735292  1.041434  \n",
       "2013-01-01 07:00:00  0.539797  0.391297  \n",
       "2013-01-01 07:30:00  0.278477  1.339480  \n",
       "2013-01-01 08:00:00  0.067382  1.230345  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Fc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-21 02:30:00</th>\n",
       "      <td>1.223779</td>\n",
       "      <td>0.961175</td>\n",
       "      <td>1.041231</td>\n",
       "      <td>0.636519</td>\n",
       "      <td>0.516030</td>\n",
       "      <td>1.089910</td>\n",
       "      <td>2.088809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-21 03:00:00</th>\n",
       "      <td>0.951709</td>\n",
       "      <td>0.937124</td>\n",
       "      <td>0.965334</td>\n",
       "      <td>0.608987</td>\n",
       "      <td>0.615727</td>\n",
       "      <td>0.755521</td>\n",
       "      <td>0.552344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-21 04:00:00</th>\n",
       "      <td>1.138612</td>\n",
       "      <td>1.000614</td>\n",
       "      <td>0.964442</td>\n",
       "      <td>0.657209</td>\n",
       "      <td>0.586111</td>\n",
       "      <td>0.662852</td>\n",
       "      <td>1.241243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-21 08:00:00</th>\n",
       "      <td>0.546598</td>\n",
       "      <td>0.635507</td>\n",
       "      <td>0.659244</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>0.463615</td>\n",
       "      <td>0.319815</td>\n",
       "      <td>0.410104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-21 08:30:00</th>\n",
       "      <td>-0.081457</td>\n",
       "      <td>0.215423</td>\n",
       "      <td>-0.397104</td>\n",
       "      <td>-0.311811</td>\n",
       "      <td>-0.030562</td>\n",
       "      <td>-0.427432</td>\n",
       "      <td>-0.338063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1         2         3         4  \\\n",
       "DateTime                                                                \n",
       "2013-03-21 02:30:00  1.223779  0.961175  1.041231  0.636519  0.516030   \n",
       "2013-03-21 03:00:00  0.951709  0.937124  0.965334  0.608987  0.615727   \n",
       "2013-03-21 04:00:00  1.138612  1.000614  0.964442  0.657209  0.586111   \n",
       "2013-03-21 08:00:00  0.546598  0.635507  0.659244  0.228027  0.463615   \n",
       "2013-03-21 08:30:00 -0.081457  0.215423 -0.397104 -0.311811 -0.030562   \n",
       "\n",
       "                            5        Fc  \n",
       "DateTime                                 \n",
       "2013-03-21 02:30:00  1.089910  2.088809  \n",
       "2013-03-21 03:00:00  0.755521  0.552344  \n",
       "2013-03-21 04:00:00  0.662852  1.241243  \n",
       "2013-03-21 08:00:00  0.319815  0.410104  \n",
       "2013-03-21 08:30:00 -0.427432 -0.338063  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvar = ['0','1','2','3','4','5']\n",
    "yvar = 'Fc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3824, 7) Test: (1367, 7)\n",
      "X_train: (3824, 6) y_train: (3824,)\n",
      "Index(['0', '1', '2', '3', '4', '5'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "print('Train:', train_df.shape, 'Test:', test_df.shape)\n",
    "\n",
    "X_train, y_train = train_df[Xvar], train_df[yvar]\n",
    "X_test, y_test = test_df[Xvar], test_df[yvar]\n",
    "\n",
    "print('X_train:', X_train.shape, 'y_train:', y_train.shape)\n",
    "print(X_train.keys())\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "X_train_scaled = scaler1.fit_transform(X_train)\n",
    "scaler2 = StandardScaler()\n",
    "y_train_scaled = scaler2.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_train_std, y_train_mean = np.std(y_train.values), np.mean(y_train.values)\n",
    "\n",
    "scaler3 = StandardScaler()\n",
    "X_test_scaled = scaler3.fit_transform(X_test)\n",
    "scaler4 = StandardScaler()\n",
    "y_test_scaled = scaler4.fit_transform(y_test.values.reshape(-1,1))\n",
    "y_test_std, y_test_mean = np.std(y_test.values), np.mean(y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3824, 6), (3824,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, y_train_scaled.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3824, 7) (1367, 7)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = np.column_stack((X_train_scaled, y_train_scaled.squeeze()))\n",
    "dataset_test = np.column_stack((X_test_scaled, y_test_scaled.squeeze()))\n",
    "\n",
    "print(dataset_train.shape, dataset_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSTEPS = 5\n",
    "NFEATURES = len(Xvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3820, 5, 6) (3820,)\n"
     ]
    }
   ],
   "source": [
    "# convert into input/output sequences\n",
    "dataset_trainX, dataset_trainy = split_sequences(dataset_train, n_steps=NSTEPS)\n",
    "print(dataset_trainX.shape, dataset_trainy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.21976292,  0.3395728 , -0.37986877, ...,  0.86154897,\n",
       "        0.78428415,  0.50438018])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_trainy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "optimizers.Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1910 samples, validate on 1910 samples\n",
      "Epoch 1/200\n",
      " - 6s - loss: 0.8646 - val_loss: 1.1297\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.8589 - val_loss: 1.1120\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.8436 - val_loss: 1.0304\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.7984 - val_loss: 0.8562\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.7483 - val_loss: 0.7793\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.7095 - val_loss: 0.7071\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.6769 - val_loss: 0.6718\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.6566 - val_loss: 0.6510\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.6401 - val_loss: 0.6324\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.6262 - val_loss: 0.6174\n",
      "Epoch 11/200\n",
      " - 1s - loss: 0.6127 - val_loss: 0.6027\n",
      "Epoch 12/200\n",
      " - 1s - loss: 0.6010 - val_loss: 0.5907\n",
      "Epoch 13/200\n",
      " - 1s - loss: 0.5900 - val_loss: 0.5797\n",
      "Epoch 14/200\n",
      " - 1s - loss: 0.5799 - val_loss: 0.5699\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.5708 - val_loss: 0.5609\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.5623 - val_loss: 0.5526\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.5542 - val_loss: 0.5448\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.5471 - val_loss: 0.5377\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.5405 - val_loss: 0.5312\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.5344 - val_loss: 0.5252\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.5288 - val_loss: 0.5196\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.5237 - val_loss: 0.5145\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.5188 - val_loss: 0.5097\n",
      "Epoch 24/200\n",
      " - 1s - loss: 0.5144 - val_loss: 0.5053\n",
      "Epoch 25/200\n",
      " - 1s - loss: 0.5102 - val_loss: 0.5012\n",
      "Epoch 26/200\n",
      " - 1s - loss: 0.5064 - val_loss: 0.4975\n",
      "Epoch 27/200\n",
      " - 1s - loss: 0.5027 - val_loss: 0.4939\n",
      "Epoch 28/200\n",
      " - 1s - loss: 0.4992 - val_loss: 0.4906\n",
      "Epoch 29/200\n",
      " - 1s - loss: 0.4957 - val_loss: 0.4874\n",
      "Epoch 30/200\n",
      " - 1s - loss: 0.4927 - val_loss: 0.4844\n",
      "Epoch 31/200\n",
      " - 1s - loss: 0.4897 - val_loss: 0.4817\n",
      "Epoch 32/200\n",
      " - 1s - loss: 0.4868 - val_loss: 0.4789\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.4846 - val_loss: 0.4767\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.4821 - val_loss: 0.4746\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.4799 - val_loss: 0.4727\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.4777 - val_loss: 0.4710\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.4756 - val_loss: 0.4692\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.4739 - val_loss: 0.4678\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.4723 - val_loss: 0.4665\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.4709 - val_loss: 0.4652\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.4696 - val_loss: 0.4641\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.4682 - val_loss: 0.4628\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.4670 - val_loss: 0.4615\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.4659 - val_loss: 0.4607\n",
      "Epoch 45/200\n",
      " - 1s - loss: 0.4646 - val_loss: 0.4598\n",
      "Epoch 46/200\n",
      " - 1s - loss: 0.4637 - val_loss: 0.4585\n",
      "Epoch 47/200\n",
      " - 1s - loss: 0.4629 - val_loss: 0.4577\n",
      "Epoch 48/200\n",
      " - 1s - loss: 0.4620 - val_loss: 0.4570\n",
      "Epoch 49/200\n",
      " - 1s - loss: 0.4612 - val_loss: 0.4561\n",
      "Epoch 50/200\n",
      " - 1s - loss: 0.4605 - val_loss: 0.4550\n",
      "Epoch 51/200\n",
      " - 1s - loss: 0.4601 - val_loss: 0.4547\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.4593 - val_loss: 0.4536\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.4585 - val_loss: 0.4529\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.4579 - val_loss: 0.4522\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.4572 - val_loss: 0.4517\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.4564 - val_loss: 0.4513\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.4557 - val_loss: 0.4509\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.4552 - val_loss: 0.4502\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.4547 - val_loss: 0.4499\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.4542 - val_loss: 0.4495\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.4538 - val_loss: 0.4490\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.4533 - val_loss: 0.4488\n",
      "Epoch 63/200\n",
      " - 1s - loss: 0.4528 - val_loss: 0.4485\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.4525 - val_loss: 0.4481\n",
      "Epoch 65/200\n",
      " - 1s - loss: 0.4521 - val_loss: 0.4479\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.4518 - val_loss: 0.4480\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.4515 - val_loss: 0.4478\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.4512 - val_loss: 0.4474\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.4509 - val_loss: 0.4480\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.4504 - val_loss: 0.4476\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.4500 - val_loss: 0.4475\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.4499 - val_loss: 0.4478\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.4496 - val_loss: 0.4474\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.4493 - val_loss: 0.4478\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.4489 - val_loss: 0.4474\n",
      "Epoch 76/200\n",
      " - 1s - loss: 0.4486 - val_loss: 0.4475\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.4484 - val_loss: 0.4477\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.4481 - val_loss: 0.4476\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.4479 - val_loss: 0.4477\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.4477 - val_loss: 0.4479\n",
      "Epoch 81/200\n",
      " - 1s - loss: 0.4476 - val_loss: 0.4479\n",
      "Epoch 82/200\n",
      " - 1s - loss: 0.4473 - val_loss: 0.4480\n",
      "Epoch 83/200\n",
      " - 1s - loss: 0.4470 - val_loss: 0.4478\n",
      "Epoch 84/200\n",
      " - 1s - loss: 0.4468 - val_loss: 0.4478\n",
      "Epoch 85/200\n",
      " - 1s - loss: 0.4466 - val_loss: 0.4481\n",
      "Epoch 86/200\n",
      " - 1s - loss: 0.4464 - val_loss: 0.4476\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.4463 - val_loss: 0.4479\n",
      "Epoch 88/200\n",
      " - 1s - loss: 0.4461 - val_loss: 0.4483\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.4458 - val_loss: 0.4479\n",
      "Epoch 90/200\n",
      " - 1s - loss: 0.4457 - val_loss: 0.4484\n",
      "Epoch 91/200\n",
      " - 1s - loss: 0.4454 - val_loss: 0.4481\n",
      "Epoch 92/200\n",
      " - 1s - loss: 0.4453 - val_loss: 0.4484\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.4450 - val_loss: 0.4479\n",
      "Epoch 94/200\n",
      " - 1s - loss: 0.4450 - val_loss: 0.4485\n",
      "Epoch 95/200\n",
      " - 1s - loss: 0.4446 - val_loss: 0.4482\n",
      "Epoch 96/200\n",
      " - 1s - loss: 0.4446 - val_loss: 0.4484\n",
      "Epoch 97/200\n",
      " - 1s - loss: 0.4443 - val_loss: 0.4483\n",
      "Epoch 98/200\n",
      " - 1s - loss: 0.4442 - val_loss: 0.4484\n",
      "Epoch 99/200\n",
      " - 1s - loss: 0.4440 - val_loss: 0.4485\n",
      "Epoch 100/200\n",
      " - 1s - loss: 0.4438 - val_loss: 0.4484\n",
      "Epoch 101/200\n",
      " - 1s - loss: 0.4436 - val_loss: 0.4484\n",
      "Epoch 102/200\n",
      " - 1s - loss: 0.4435 - val_loss: 0.4486\n",
      "Epoch 103/200\n",
      " - 1s - loss: 0.4432 - val_loss: 0.4488\n",
      "Epoch 104/200\n",
      " - 1s - loss: 0.4431 - val_loss: 0.4485\n",
      "Epoch 105/200\n",
      " - 1s - loss: 0.4429 - val_loss: 0.4483\n",
      "Epoch 106/200\n",
      " - 1s - loss: 0.4430 - val_loss: 0.4485\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.4426 - val_loss: 0.4490\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.4423 - val_loss: 0.4487\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.4421 - val_loss: 0.4489\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.4419 - val_loss: 0.4485\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.4418 - val_loss: 0.4488\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.4416 - val_loss: 0.4485\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.4415 - val_loss: 0.4489\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.4411 - val_loss: 0.4487\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.4412 - val_loss: 0.4490\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.4408 - val_loss: 0.4496\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.4405 - val_loss: 0.4498\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.4404 - val_loss: 0.4487\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.4403 - val_loss: 0.4488\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.4404 - val_loss: 0.4487\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.4399 - val_loss: 0.4495\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.4396 - val_loss: 0.4494\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.4393 - val_loss: 0.4490\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.4395 - val_loss: 0.4486\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.4390 - val_loss: 0.4491\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.4389 - val_loss: 0.4491\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.4386 - val_loss: 0.4492\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.4385 - val_loss: 0.4492\n",
      "Epoch 129/200\n",
      " - 1s - loss: 0.4382 - val_loss: 0.4491\n",
      "Epoch 130/200\n",
      " - 1s - loss: 0.4381 - val_loss: 0.4488\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.4380 - val_loss: 0.4492\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.4377 - val_loss: 0.4494\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.4374 - val_loss: 0.4494\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.4372 - val_loss: 0.4494\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.4372 - val_loss: 0.4491\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.4369 - val_loss: 0.4492\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.4367 - val_loss: 0.4498\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.4364 - val_loss: 0.4499\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.4361 - val_loss: 0.4494\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.4359 - val_loss: 0.4496\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.4359 - val_loss: 0.4495\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.4356 - val_loss: 0.4496\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.4355 - val_loss: 0.4494\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.4353 - val_loss: 0.4499\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.4349 - val_loss: 0.4500\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.4348 - val_loss: 0.4497\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.4345 - val_loss: 0.4501\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.4343 - val_loss: 0.4502\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.4341 - val_loss: 0.4497\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.4339 - val_loss: 0.4499\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.4337 - val_loss: 0.4504\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.4336 - val_loss: 0.4504\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.4332 - val_loss: 0.4502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      " - 0s - loss: 0.4330 - val_loss: 0.4501\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.4329 - val_loss: 0.4505\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.4326 - val_loss: 0.4505\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.4325 - val_loss: 0.4502\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.4323 - val_loss: 0.4503\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.4321 - val_loss: 0.4506\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.4319 - val_loss: 0.4507\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.4316 - val_loss: 0.4503\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.4314 - val_loss: 0.4507\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.4313 - val_loss: 0.4504\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.4309 - val_loss: 0.4505\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.4307 - val_loss: 0.4504\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.4307 - val_loss: 0.4503\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.4303 - val_loss: 0.4505\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.4301 - val_loss: 0.4507\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.4298 - val_loss: 0.4506\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.4296 - val_loss: 0.4508\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.4293 - val_loss: 0.4508\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.4291 - val_loss: 0.4511\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.4288 - val_loss: 0.4510\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.4283 - val_loss: 0.4514\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.4281 - val_loss: 0.4510\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.4279 - val_loss: 0.4509\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.4278 - val_loss: 0.4510\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.4273 - val_loss: 0.4516\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.4272 - val_loss: 0.4514\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.4270 - val_loss: 0.4521\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.4263 - val_loss: 0.4521\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.4264 - val_loss: 0.4516\n",
      "Epoch 183/200\n",
      " - 1s - loss: 0.4260 - val_loss: 0.4522\n",
      "Epoch 184/200\n",
      " - 1s - loss: 0.4258 - val_loss: 0.4521\n",
      "Epoch 185/200\n",
      " - 1s - loss: 0.4256 - val_loss: 0.4523\n",
      "Epoch 186/200\n",
      " - 1s - loss: 0.4251 - val_loss: 0.4529\n",
      "Epoch 187/200\n",
      " - 1s - loss: 0.4250 - val_loss: 0.4522\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.4250 - val_loss: 0.4525\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.4246 - val_loss: 0.4525\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.4242 - val_loss: 0.4533\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.4239 - val_loss: 0.4531\n",
      "Epoch 192/200\n",
      " - 1s - loss: 0.4239 - val_loss: 0.4532\n",
      "Epoch 193/200\n",
      " - 1s - loss: 0.4235 - val_loss: 0.4532\n",
      "Epoch 194/200\n",
      " - 1s - loss: 0.4235 - val_loss: 0.4538\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.4230 - val_loss: 0.4535\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.4229 - val_loss: 0.4540\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.4226 - val_loss: 0.4544\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.4223 - val_loss: 0.4542\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.4221 - val_loss: 0.4546\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.4220 - val_loss: 0.4539\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape=(NSTEPS, NFEATURES), activation='relu'))\n",
    "model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(3, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))   \n",
    "model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "history = model.fit(dataset_trainX, dataset_trainy, \n",
    "                    validation_split=0.5, shuffle=False, \n",
    "                    epochs=200, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_testX, dataset_testy = split_sequences(dataset_test, n_steps=NSTEPS)\n",
    "yhat_test = model.predict(dataset_testX, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test.shape, dataset_testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yhat_test.squeeze(), dataset_testy, 'o')\n",
    "plt.plot([-2,2],[-2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(yhat_test.squeeze(), dataset_testy, mincnt=2, gridsize=30, bins='log')\n",
    "plt.plot([-2,2],[-2,2])\n",
    "\n",
    "#plt.plot([-2,2], [-2,2], 'r', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM: ')\n",
    "diagnostic_stats(dataset_testy*y_test_std + y_test_mean, \n",
    "                 yhat_test.squeeze()*y_test_std + y_test_mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_randomsearchcv(model, params, X_train, y_train, n_folds, random_state=0):\n",
    "    clf = RandomizedSearchCV(model, params, random_state=random_state, cv=n_folds, return_train_score=True, \n",
    "                         scoring=\"neg_mean_squared_error\")\n",
    "    search = clf.fit(X_train, y_train)\n",
    "    rmse= np.sqrt(-search.best_score_)\n",
    "    return rmse, search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optz\n",
    "params = dict(num_leaves = randint(2,10), \n",
    "              n_estimators = randint(100,1000),\n",
    "              learning_rate = uniform(1e-3, 1),\n",
    "              min_data_in_leaf = randint(2,10),\n",
    "              objective=['regression'])\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor()\n",
    "score_lgb, model_lgb = rmsle_randomsearchcv(model_lgb, params, dataset_train[:,:-1], \n",
    "                                            dataset_train[:,-1], 5)\n",
    "print(\"\\nLightGBM score: {:.4f} (+/-{:.4f})\\n\".format(score_lgb.mean(), score_lgb.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_lgbm = model_lgb.predict(dataset_test[:,:-1])\n",
    "\n",
    "print('LGBM')\n",
    "diagnostic_stats(dataset_test[:,-1]*y_test_std + y_test_mean, \n",
    "                 yhat_lgbm*y_test_std + y_test_mean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yhat_lgbm.squeeze(), dataset_test[:,-1], 'o')\n",
    "plt.plot([-2,2],[-2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM: ')\n",
    "diagnostic_stats(dataset_testy*y_test_std + y_test_mean, \n",
    "                 yhat_test.squeeze()*y_test_std + y_test_mean);\n",
    "print('LGBM')\n",
    "diagnostic_stats(dataset_test[:,-1]*y_test_std + y_test_mean, \n",
    "                 yhat_lgbm*y_test_std + y_test_mean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
